{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Загрузка исходных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные данные в формате .txt. \n",
    "\n",
    "Перечень полей:\n",
    "\n",
    "id|||состояние (действует/не действует)|||Номер заявки|||Категории МПК|||Дата подачи заявки|||Дата регистрации|||Автор|||Правообладатель|||Название|||Аннотация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='.\\\\input_data\\\\'\n",
    "file_name='!all_docs.txt'\n",
    "vpw_name='all_vpw.txt'\n",
    "\n",
    "# --- open single file !all_docs.txt then reorganize for vowpal wabbit format ---\n",
    "\n",
    "with open(data_dir+file_name, \"r\") as fp:\n",
    "\n",
    "    patents_info_lines = fp.readlines()\n",
    "    \n",
    "    with open(data_dir+vpw_name, 'w') as vpw_file:\n",
    "        for patent in patents_info_lines:\n",
    "            patent_list=patent.decode('utf8').split('|||')         \n",
    "            \n",
    "            mpks=patent_list[len(patent_list)-7].split('|')\n",
    "            \n",
    "            for mpk in mpks:\n",
    "                \n",
    "                mpk=mpk.strip()[:-10]  # delete (2006.01) and so on at the end of mpk\n",
    "                mpk_list=[]\n",
    "                mpk_list=mpk.split()\n",
    "                mpks_main_vpw=mpks_main_vpw+mpk_list[0] + ' '\n",
    "                mpks_vpw=mpks_vpw + mpk.replace(\" \", \"_\") + ' '\n",
    "                      \n",
    "            authors=patent_list[len(patent_list)-4].split('|')\n",
    "            authors_vpw=''\n",
    "            for author in authors:\n",
    "                author=author.replace(\"(RU)\", \"\").rstrip()\n",
    "                authors_vpw=authors_vpw + author.replace(\" \", \"_\") + ' '\n",
    "            # -\n",
    "            \n",
    "             # - assignees rewrite -\n",
    "            assignees=patent_list[len(patent_list)-3].split('|')\n",
    "            assignees_vpw=''\n",
    "            for assignee in assignees:\n",
    "                assignee=assignee.replace(\"(RU)\", \"\").rstrip()\n",
    "                assignees_vpw=assignees_vpw + assignee.replace(\" \", \"_\") + ' '\n",
    "            # -\n",
    "            \n",
    "            #  - title and abstract rewrite   - delete symbol \"|\"\n",
    "            title=patent_list[len(patent_list)-2].replace(\"|\", \"\")\n",
    "            abstract=patent_list[len(patent_list)-1].replace(\"|\", \"\")\n",
    "            \n",
    "            new_line=''\n",
    "            new_line=patent_list[0]+' |type RUPM |' + 'status '+patent_list[1] + ' |mpk_main ' + mpks_main_vpw + '|mpk_full ' + mpks_vpw +\\\n",
    "            '|date_app ' + patent_list[len(patent_list)-6] + ' |date_pub ' + patent_list[len(patent_list)-5] + ' |authors ' + authors_vpw +\\\n",
    "            '|assignee ' + assignees_vpw + '|text ' + title + ' '+ abstract\n",
    "            vpw_file.write(new_line.encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процедуры предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='.\\\\input_data\\\\'\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# -- стоп слова\n",
    "stop_file=\"stop_extend.txt\"\n",
    "russian_stopwords = stopwords.words(\"russian\")   \n",
    "rsw = [unicode(word) for word in russian_stopwords]\n",
    "rsw = rsw + open(data_dir+stop_file, \"r\").read().decode('cp1251').split()\n",
    "\n",
    "#-- файлы с \"белыми списками\" - слова из названий программ импортозамещения: авиастроение, строительный_транспорт и т.п.\n",
    "importTitles_unigrams_file=\"import_titles_unigrams.txt\"\n",
    "importTitles_bigrams_file=\"import_titles_bigrams.txt\"\n",
    "imp_uni=open(data_dir+importTitles_unigrams_file, \"r\").read().decode('cp1251').split()\n",
    "imp_bi=open(data_dir+importTitles_bigrams_file, \"r\").read().decode('cp1251').split()\n",
    "\n",
    "tokenizer = RegexpTokenizer(u'[A-Z|a-z|А-Я|Ё|а-я|ё|]+') # only words         \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "    \n",
    "def text_preprocess(plain_text):    \n",
    "    intermediate = tokenizer.tokenize(plain_text.lower())\n",
    "    intermediate = [i for i in intermediate if len(i)>2]\n",
    "    intermediate = [morph.parse(i)[0].normal_form for i in intermediate]\n",
    "    preprocessed_text = [i for i in intermediate if i not in rsw] \n",
    "    return preprocessed_text\n",
    "\n",
    "def get_counted_unigrams(preprocessed_text):\n",
    "    counts_uni = Counter(preprocessed_text)  # счетчик частот униграмм  \n",
    "    unigrams_count_line='' # unigrams in line with counts\n",
    "    import_unigrams_count_line=''  # отдельно выделяются слова из белого списка униграмм программы импортозамещения (из названия программ, например: аэродромный, автомобильный и т.п.)\n",
    "    for char in counts_uni.keys():\n",
    "        if char in imp_uni:    # если слово содержится в \"белом\" списке униграм программы импортозамещения (например: фармацевтический, авиастроение и т.п.)\n",
    "            import_unigrams_count_line=import_unigrams_count_line+char+':'+str(counts_uni[char])+' '\n",
    "        else:\n",
    "            unigrams_count_line=unigrams_count_line+char+':'+str(counts_uni[char])+' '\n",
    "    return import_unigrams_count_line, unigrams_count_line\n",
    "\n",
    "def get_counted_bigrams(preprocessed_text): # Наиболее частотные биграммы с частотой большей 2. \n",
    "    # (также потом к \"биграммам относились унигаммы с названием отрасли. например: транспортный, химический и т.д.\")\n",
    "    \n",
    "    #- bigrams cut-off   --  сколько наиболее частотный биграмм брать, на какой частоте отсекать для  включения в словарь\n",
    "    bigram_most_common_count=15\n",
    "    bigramm_cutoff_value=2\n",
    "    bigrams_count_line='' # bigrams in line with counts\n",
    "    import_bigrams_count_line=''\n",
    "    biword =  [b for b in nltk.bigrams(preprocessed_text)]\n",
    "    counts_bi = Counter(biword)  # счетчик частот биграмм\n",
    "    import_bigrams_count_line=''\n",
    "    for char in counts_bi.keys():\n",
    "        char_='_'.join(char)\n",
    "        bigrams_count_line='' # bigrams in line with counts\n",
    "        if char_ in imp_bi:\n",
    "            import_bigrams_count_line=import_bigrams_count_line+char_+':'+str(counts_bi[char])+' '\n",
    "        else:\n",
    "            bigrams_common=counts_bi.most_common(bigram_most_common_count)\n",
    "            \n",
    "            for bigram in bigrams_common:\n",
    "                if bigram[1]>=bigramm_cutoff_value:\n",
    "                    bigrams_count_line=bigrams_count_line+ '_'.join(bigram[0]) +':' +str(bigram[1]) +' ' \n",
    "    return import_bigrams_count_line, bigrams_count_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка файла в формате .vpw для основного файла данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='.\\\\input_data\\\\'\n",
    "vpw_name='all_vpw2.txt'\n",
    "vpw_lemmas_name='all_vpw_lemmas_bigrams2.txt'\n",
    "\n",
    "bigram_most_common_count=15\n",
    "bigramm_cutoff_value=3\n",
    "\n",
    "import io\n",
    "import pymorphy2\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# --- Main text preprocessing (counted unigrams, bigrams) ---\n",
    "with open(data_dir+vpw_name, \"r\") as f:\n",
    "    \n",
    "    patents_info_lines = f.readlines()\n",
    "        \n",
    "    with io.open(data_dir+vpw_lemmas_name, 'w', encoding='utf8') as vpw_lemmas_file:  #file to write lemmas and bigramms\n",
    "        for patent in patents_info_lines:\n",
    "            patent_list=patent.decode('utf8').split('|')\n",
    "            text = patent_list[len(patent_list)-1][5:]  # patent text (title+abstract) is the last one;  [5:] - вырезаем из начала \"text \"\n",
    "            preprocessed_text=text_preprocess(text) \n",
    "            import_unigrams_count_line, unigrams_count_line=get_counted_unigrams(preprocessed_text)\n",
    "            import_bigrams_count_line, bigrams_count_line=get_counted_bigrams(preprocessed_text)\n",
    "        \n",
    "            # modalities not changed:\n",
    "            old_line='|'.join(patent_list[:-1])\n",
    "            new_line=old_line+' |text ' + unigrams_count_line + ' |bigrams ' + import_unigrams_count_line + import_bigrams_count_line + bigrams_count_line\n",
    "            vpw_lemmas_file.write(new_line)\n",
    "            vpw_lemmas_file.write(unicode('\\n'))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 22 файлов .txt по отраслям импортозамещения - подготовить для каждого файл в формате .vpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----   From Import program prepare 22 different vowpal wabbit files--- \n",
    "# ----- !!! Prepare 22 different files !! ----\n",
    "import io\n",
    "import glob\n",
    "import codecs\n",
    "\n",
    "data_dir='.\\\\import_vpw_and_batches\\\\'\n",
    "\n",
    "files = glob.glob(\"import_text_files/*.txt\")\n",
    "for imp_file in files:\n",
    "    file_name=imp_file.split(\"\\\\\")[1].split(\".\")[0].replace(' ', '_')  # get file name like \"aerodromn_tehn\"\n",
    "    file_name=file_name+\".txt\"\n",
    "    with codecs.open(imp_file, 'r', 'cp1251') as f:\n",
    "        import_file_in_line = \" \".join(line.strip().replace('|', ' ') for line in f)\n",
    "        preprocessed_text=text_preprocess(import_file_in_line)\n",
    "        import_unigrams_count_line, unigrams_count_line=get_counted_unigrams(preprocessed_text)\n",
    "        import_bigrams_count_line, bigrams_count_line=get_counted_bigrams(preprocessed_text)\n",
    "        with io.open(data_dir+file_name, 'w', encoding='utf8') as f_out:\n",
    "            f_out.write('|text ' + unigrams_count_line + ' |bigrams ' + import_unigrams_count_line + import_bigrams_count_line + bigrams_count_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для 22 файлов .vpw (по отраслям импортозамещения) создать 22 словаря, по которым потом будем потом вводить регуляризаторы сглаживания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Build 22 Bigartm batches and 22 dictionary for IMPORT files\n",
    "import artm\n",
    "import glob\n",
    "\n",
    "data_dir='.\\\\import_vpw_and_batches\\\\'\n",
    "\n",
    "files = glob.glob(\"import_vpw_and_batches/*.txt\")\n",
    "for imp_file in files:\n",
    "    file_name_txt=imp_file.split(\"\\\\\")[1]\n",
    "    file_name=file_name_txt.split('.')[0]\n",
    "    target_folder=file_name+'_batches'\n",
    "    batch_vectorizer_import = artm.BatchVectorizer(data_path=data_dir+file_name_txt,\n",
    "                                        data_format='vowpal_wabbit',\n",
    "                                        target_folder=data_dir+target_folder,\n",
    "                                        batch_size=100)\n",
    "    import_dictionary = artm.Dictionary()\n",
    "    dictionary_name=file_name+'_dictionary'\n",
    "    dictionary_name_txt=dictionary_name+'.txt'\n",
    "    import_dictionary.gather(data_path=data_dir+target_folder)\n",
    "    import_dictionary.save(dictionary_path=data_dir+dictionary_name)\n",
    "    import_dictionary.save_text(dictionary_path=data_dir+dictionary_name_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- create batches and MAIN dictionary \n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!! We need it only once !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \n",
    "import artm\n",
    "\n",
    "data_dir='.\\\\input_data\\\\'\n",
    "data_file='all_vpw_lemmas_bigrams.txt'\n",
    "\n",
    "# ----  BIGARTM !!! ------\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path=data_dir+data_file,\n",
    "                                        data_format='vowpal_wabbit',\n",
    "                                        target_folder='collection_batches',\n",
    "                                        class_ids=['text', 'bigrams'] #, 'mpk_main', 'mpk_full']\n",
    "                                       )\n",
    "\n",
    "main_dictionary = artm.Dictionary()\n",
    "main_dictionary.gather(data_path='collection_batches')\n",
    "main_dictionary.save(dictionary_path='collection_batches/main_dictionary')\n",
    "main_dictionary.save_text(dictionary_path='collection_batches/main_dictionary.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  -- Filter MAIN dictionary from not specifield words  --\n",
    "#сохранить токены с df=[1,3500)\n",
    "\n",
    "main_dictionary.load_text(dictionary_path='collection_batches/main_dictionary.txt')\n",
    "main_dictionary.filter(class_id='text', min_df=1, max_df=3500)   #сохранить токены с df=[1,3500) !! предварительно сохранив копию, т.к. фильтр вносит изменения сразу в файл\n",
    "main_dictionary.save_text(dictionary_path='collection_batches/main_dictionary.txt')\n",
    "\n",
    "# просматриваем получившийся файл в панде\n",
    "data_dir='.\\\\collection_batches\\\\'\n",
    "data_file='main_dictionary_for_pandas.txt'  # отличается от main_dictionary.txt удалением первых двух служебных строк\n",
    "header=['token', 'class_id', 'value', 'tf', 'df']\n",
    "main_pd = pd.read_csv(data_dir+data_file, delimiter=',', names=header, encoding=\"utf8\")\n",
    "main_pd = main_pd.sort_values(by ='df', ascending=False)\n",
    "main_pd.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------  Edit Import Dictionary  --  Set value= 2.0 for all import-words\n",
    "# ------- Edit Main Dictionary    --  Set value -1.0 for all words\n",
    "\n",
    "data_dir='.\\\\collection_batches\\\\'\n",
    "import_dir='.\\\\import_batches\\\\'\n",
    "\n",
    "data_file='main_dictionary_for_pandas.txt'\n",
    "import_file='import_dictionary_filtered_from_pandas.txt'\n",
    "\n",
    "header=['token', 'class_id', 'value', 'tf', 'df']\n",
    "\n",
    "main_pd = pd.read_csv(data_dir+data_file, delimiter=',', names=header, encoding=\"utf8\")\n",
    "import_pd = pd.read_csv(import_dir+import_file, delimiter=',', names=header, encoding=\"utf8\")\n",
    "import_list = import_pd[\"token\"].tolist()[2:]  # убираем первые 2 строчки\n",
    "main_with_import_pd=main_pd[main_pd['token'].isin(import_list)]  # строки main_dict у которых token из import словаря\n",
    "main_with_import_pd.loc[:,'value']=2\n",
    "main_others_pd=main_pd[~main_pd['token'].isin(import_list)]  # токены не из импорт словаря\n",
    "main_others_pd.loc[:,'value']=-1\n",
    "\n",
    "main_edited_final_pd = pd.concat([main_with_import_pd, main_others_pd], ignore_index=True, sort =False)\n",
    "\n",
    "out_file= 'edited_dictionary_2_-1.txt'\n",
    "main_edited_final_pd.to_csv(data_dir+out_file, sep=',', encoding='utf8', index=False, header=False)\n",
    "\n",
    "# !!!  в финальном файле руками добавить пробелы перед запятыми и первые две строчки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dictionary.save(dictionary_path='collection_batches/main_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
