{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='collection_batches',\n",
    "                                        data_format='batches')\n",
    "main_dictionary = artm.Dictionary()\n",
    "#main_dictionary.load_text(dictionary_path='collection_batches/edited_dictionary_2_-1.txt')\n",
    "main_dictionary.load_text(dictionary_path='collection_batches/main_dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_scores(model):\n",
    "    # добавляем скоры модели, которые нас будут интересовать\n",
    "    model.scores.add(artm.PerplexityScore(name='PerplexityScore',\n",
    "                                         # class_ids='text',\n",
    "                                          dictionary = main_dictionary))\n",
    " \n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore_text', \n",
    "                                           class_id='text',\n",
    "                                           topic_names = domain_topics))\n",
    "                    \n",
    "      \n",
    "    model.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore_bigrams', \n",
    "                                           class_id='bigrams',\n",
    "                                           topic_names = domain_topics)\n",
    "                    )                      \n",
    "    \n",
    "    model.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore',\n",
    "                                           topic_names = domain_topics)\n",
    "                    )\n",
    "    \n",
    "    model.scores.add(artm.TopTokensScore(name='Domain_Top10_Tokens', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='text')\n",
    "                                         #topic_names = domain_topics)\n",
    "                    )\n",
    "        \n",
    "    model.scores.add(artm.TopTokensScore(name='Bigrams_Top10_Tokens', \n",
    "                                         num_tokens=15, \n",
    "                                         dictionary = main_dictionary,\n",
    "                                         class_id='bigrams')\n",
    "                                       #  topic_names = domain_topics) \n",
    "                    )\n",
    "\n",
    "\n",
    "    model.scores.add(artm.TopicKernelScore(name='DomainTopicKernelScore', \n",
    "                                           probability_mass_threshold=0.25, \n",
    "                                           #dictionary = main_dictionary,\n",
    "                                           class_id='text',\n",
    "                                           topic_names = domain_topics)\n",
    "                    )\n",
    "                                          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_SparsityPhiScores(model, i, topic_name):\n",
    "    name_text='SparsityPhiScore_text'+ i\n",
    "    name_bigrams='SparsityPhiScore_bigrams'+ i\n",
    "    model.scores.add(artm.SparsityPhiScore(name=name_text, \n",
    "                                           class_id='text',\n",
    "                                           topic_names = topic_name))\n",
    "    model.scores.add(artm.SparsityPhiScore(name=name_bigrams, \n",
    "                                           class_id='bigrams',\n",
    "                                           topic_names = topic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_TopicKernelScore(model, i, topic_name):\n",
    "    name_text='DomainTopicKernelScore_text'+ i\n",
    "    name_bigrams='DomainTopicKernelScore_bigrams'+ i\n",
    "    model.scores.add(artm.TopicKernelScore(name=name_text, \n",
    "                                           probability_mass_threshold=0.25,\n",
    "                                           class_id='text',\n",
    "                                           topic_names = topic_name))\n",
    "    model.scores.add(artm.TopicKernelScore(name=name_bigrams,\n",
    "                                           probability_mass_threshold=0.25,\n",
    "                                           class_id='bigrams',\n",
    "                                           topic_names = topic_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Set_Multiple_Scores():\n",
    "    \n",
    "    for i in range(1,23):\n",
    "        set_SparsityPhiScores(model, str(i), 'd'+str(i))\n",
    "        set_TopicKernelScore(model, str(i), 'd'+str(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "dict_aerodromn = artm.Dictionary()\n",
    "dict_aerodromn.load_text(dictionary_path=data_dir+file_list[1])\n",
    "Reg_aerodromn=artm.SmoothSparsePhiRegularizer(name='Smoth_t1', tau=tau, class_ids='text', dictionary=dict_aerodromn, topic_names='d1')\n",
    "Reg_aerodromn_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b1', tau=tau, class_ids='bigrams', dictionary=dict_aerodromn, topic_names='d1')\n",
    "    #2\n",
    "dict_automobil = artm.Dictionary()\n",
    "dict_automobil.load_text(dictionary_path=data_dir+file_list[2])\n",
    "Reg_automobil=artm.SmoothSparsePhiRegularizer(name='Smoth_t2', tau=tau, class_ids='text', dictionary=dict_automobil, topic_names='d2')\n",
    "Reg_automobil_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b2', tau=tau, class_ids='bigrams', dictionary=dict_automobil, topic_names='d2')\n",
    "    #3\n",
    "dict_aviastrien = artm.Dictionary()\n",
    "dict_aviastrien.load_text(dictionary_path=data_dir+file_list[3])\n",
    "Reg_aviastrien=artm.SmoothSparsePhiRegularizer(name='Smoth_t3', tau=tau, class_ids='text', dictionary=dict_aviastrien, topic_names='d3')\n",
    "Reg_aviastrien_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b3', tau=tau, class_ids='bigrams', dictionary=dict_aviastrien, topic_names='d3')\n",
    "#4\n",
    "dict_chern_metallurg = artm.Dictionary()\n",
    "dict_chern_metallurg.load_text(dictionary_path=data_dir+file_list[4])\n",
    "Reg_chern_metallurg=artm.SmoothSparsePhiRegularizer(name='Smoth_t4', tau=tau, class_ids='text', dictionary=dict_chern_metallurg, topic_names='d4')\n",
    "Reg_chern_metallurg_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b4', tau=tau, class_ids='bigrams', dictionary=dict_chern_metallurg, topic_names='d4')\n",
    "#5\n",
    "dict_deti = artm.Dictionary()\n",
    "dict_deti.load_text(dictionary_path=data_dir+file_list[5])\n",
    "Reg_deti=artm.SmoothSparsePhiRegularizer(name='Smoth_t5', tau=tau, class_ids='text', dictionary=dict_deti, topic_names='d5')\n",
    "Reg_deti_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b5', tau=tau, class_ids='bigrams', dictionary=dict_deti, topic_names='d5')\n",
    "#6\n",
    "dict_energetich = artm.Dictionary()\n",
    "dict_energetich.load_text(dictionary_path=data_dir+file_list[6])\n",
    "Reg_energetich=artm.SmoothSparsePhiRegularizer(name='Smoth_t6', tau=tau, class_ids='text', dictionary=dict_energetich, topic_names='d6')\n",
    "Reg_energetich_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b6', tau=tau, class_ids='bigrams', dictionary=dict_energetich, topic_names='d6')\n",
    "#7\n",
    "dict_himich = artm.Dictionary()\n",
    "dict_himich.load_text(dictionary_path=data_dir+file_list[7])\n",
    "Reg_himich=artm.SmoothSparsePhiRegularizer(name='Smoth_t7', tau=tau, class_ids='text', dictionary=dict_himich, topic_names='d7')\n",
    "Reg_himich_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b7', tau=tau, class_ids='bigrams', dictionary=dict_himich, topic_names='d7')\n",
    "#8\n",
    "dict_legkaya = artm.Dictionary()\n",
    "dict_legkaya.load_text(dictionary_path=data_dir+file_list[8])\n",
    "Reg_legkaya=artm.SmoothSparsePhiRegularizer(name='Smoth_t8', tau=tau, class_ids='text', dictionary=dict_legkaya, topic_names='d8')\n",
    "Reg_legkaya_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b8', tau=tau, class_ids='bigrams', dictionary=dict_legkaya, topic_names='d8')\n",
    "#9\n",
    "dict_lesopromishl = artm.Dictionary()\n",
    "dict_lesopromishl.load_text(dictionary_path=data_dir+file_list[9])\n",
    "Reg_lesopromishl=artm.SmoothSparsePhiRegularizer(name='Smoth_t9', tau=tau, class_ids='text', dictionary=dict_lesopromishl, topic_names='d9')\n",
    "Reg_lesopromishl_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b9', tau=tau, class_ids='bigrams', dictionary=dict_lesopromishl, topic_names='d9')\n",
    "#10\n",
    "dict_med = artm.Dictionary()\n",
    "dict_med.load_text(dictionary_path=data_dir+file_list[10])\n",
    "Reg_med=artm.SmoothSparsePhiRegularizer(name='Smoth_t10', tau=tau, class_ids='text', dictionary=dict_med, topic_names='d10')\n",
    "Reg_med_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b10', tau=tau, class_ids='bigrams', dictionary=dict_med, topic_names='d10')\n",
    "\n",
    "#11\n",
    "dict_neftegaz = artm.Dictionary()\n",
    "dict_neftegaz.load_text(dictionary_path=data_dir+file_list[11])\n",
    "Reg_neftegaz=artm.SmoothSparsePhiRegularizer(name='Smoth_t11', tau=tau, class_ids='text', dictionary=dict_neftegaz, topic_names='d11')\n",
    "Reg_neftegaz_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b11', tau=tau, class_ids='bigrams', dictionary=dict_neftegaz, topic_names='d11')\n",
    "#12\n",
    "dict_pharm = artm.Dictionary()\n",
    "dict_pharm.load_text(dictionary_path=data_dir+file_list[12])\n",
    "Reg_pharm=artm.SmoothSparsePhiRegularizer(name='Smoth_t12', tau=tau, class_ids='text', dictionary=dict_pharm, topic_names='d12')\n",
    "Reg_pharm_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b12', tau=tau, class_ids='bigrams', dictionary=dict_pharm, topic_names='d12')\n",
    "#13\n",
    "dict_pish_i_pererab= artm.Dictionary()\n",
    "dict_pish_i_pererab.load_text(dictionary_path=data_dir+file_list[13])\n",
    "Reg_pish_i_pererab=artm.SmoothSparsePhiRegularizer(name='Smoth_t13', tau=tau, class_ids='text', dictionary=dict_pish_i_pererab, topic_names='d13')\n",
    "Reg_pish_i_pererab_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b13', tau=tau, class_ids='bigrams', dictionary=dict_pish_i_pererab, topic_names='d13')\n",
    "#14\n",
    "dict_vooruzh = artm.Dictionary()\n",
    "dict_vooruzh.load_text(dictionary_path=data_dir+file_list[14])\n",
    "Reg_vooruzh=artm.SmoothSparsePhiRegularizer(name='Smoth_t14', tau=tau, class_ids='text', dictionary=dict_vooruzh, topic_names='d14')\n",
    "Reg_vooruzh_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b14', tau=tau, class_ids='bigrams', dictionary=dict_vooruzh, topic_names='d14')\n",
    "#15\n",
    "dict_radioelektron = artm.Dictionary()\n",
    "dict_radioelektron.load_text(dictionary_path=data_dir+file_list[15])\n",
    "Reg_radioelektron=artm.SmoothSparsePhiRegularizer(name='Smoth_t15', tau=tau, class_ids='text', dictionary=dict_radioelektron, topic_names='d15')\n",
    "Reg_radioelektron_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b15', tau=tau, class_ids='bigrams', dictionary=dict_radioelektron, topic_names='d15')\n",
    "#16\n",
    "dict_selskoxoz_i_lesnoe = artm.Dictionary()\n",
    "dict_selskoxoz_i_lesnoe.load_text(dictionary_path=data_dir+file_list[16])\n",
    "Reg_selskoxoz_i_lesnoe=artm.SmoothSparsePhiRegularizer(name='Smoth_t16', tau=tau, class_ids='text', dictionary=dict_selskoxoz_i_lesnoe, topic_names='d16')\n",
    "Reg_selskoxoz_i_lesnoe_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b16', tau=tau, class_ids='bigrams', dictionary=dict_selskoxoz_i_lesnoe, topic_names='d16')\n",
    "#17\n",
    "dict_stankoinstrument = artm.Dictionary()\n",
    "dict_stankoinstrument.load_text(dictionary_path=data_dir+file_list[17])\n",
    "Reg_stankoinstrument=artm.SmoothSparsePhiRegularizer(name='Smoth_t17', tau=tau, class_ids='text', dictionary=dict_stankoinstrument, topic_names='d17')\n",
    "Reg_stankoinstrument_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b17', tau=tau, class_ids='bigrams', dictionary=dict_stankoinstrument, topic_names='d17')\n",
    "#18\n",
    "dict_stroit = artm.Dictionary()\n",
    "dict_stroit.load_text(dictionary_path=data_dir+file_list[18])\n",
    "Reg_stroit=artm.SmoothSparsePhiRegularizer(name='Smoth_t18', tau=tau, class_ids='text', dictionary=dict_stroit, topic_names='d18')\n",
    "Reg_stroit_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b18', tau=tau, class_ids='bigrams', dictionary=dict_stroit, topic_names='d18')\n",
    "#19\n",
    "dict_sudostroit = artm.Dictionary()\n",
    "dict_sudostroit.load_text(dictionary_path=data_dir+file_list[19])\n",
    "Reg_sudostroit=artm.SmoothSparsePhiRegularizer(name='Smoth_t19', tau=tau, class_ids='text', dictionary=dict_sudostroit, topic_names='d19')\n",
    "Reg_sudostroit_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b19', tau=tau, class_ids='bigrams', dictionary=dict_sudostroit, topic_names='d19')\n",
    "#20\n",
    "dict_transportn = artm.Dictionary()\n",
    "dict_transportn.load_text(dictionary_path=data_dir+file_list[20])\n",
    "Reg_transportn=artm.SmoothSparsePhiRegularizer(name='Smoth_t20', tau=tau, class_ids='text', dictionary=dict_transportn, topic_names='d20')\n",
    "Reg_transportn_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b20', tau=tau, class_ids='bigrams', dictionary=dict_transportn, topic_names='d20')\n",
    "#21\n",
    "dict_tsvetn_metallurg = artm.Dictionary()\n",
    "dict_tsvetn_metallurg.load_text(dictionary_path=data_dir+file_list[21])\n",
    "Reg_tsvetn_metallurg=artm.SmoothSparsePhiRegularizer(name='Smoth_t21', tau=tau, class_ids='text', dictionary=dict_tsvetn_metallurg, topic_names='d21')\n",
    "Reg_tsvetn_metallurg_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b21', tau=tau, class_ids='bigrams', dictionary=dict_tsvetn_metallurg, topic_names='d21')\n",
    "#22\n",
    "dict_tyazh_mashinostr = artm.Dictionary()\n",
    "dict_tyazh_mashinostr.load_text(dictionary_path=data_dir+file_list[22])\n",
    "Reg_tyazh_mashinostr=artm.SmoothSparsePhiRegularizer(name='Smoth_t22', tau=tau, class_ids='text', dictionary=dict_tyazh_mashinostr, topic_names='d22')\n",
    "Reg_tyazh_mashinostr_b=artm.SmoothSparsePhiRegularizer(name='Smoth_b22', tau=tau, class_ids='bigrams', dictionary=dict_tyazh_mashinostr, topic_names='d22')\n",
    "\n",
    "model.regularizers.add(Reg_aerodromn)\n",
    "model.regularizers.add(Reg_aerodromn_b)\n",
    "model.regularizers.add(Reg_automobil)\n",
    "model.regularizers.add(Reg_automobil_b)\n",
    "model.regularizers.add(Reg_aviastrien)\n",
    "model.regularizers.add(Reg_aviastrien_b)\n",
    "model.regularizers.add(Reg_chern_metallurg)\n",
    "model.regularizers.add(Reg_chern_metallurg_b)\n",
    "model.regularizers.add(Reg_deti)\n",
    "model.regularizers.add(Reg_deti_b)\n",
    "model.regularizers.add(Reg_energetich)\n",
    "model.regularizers.add(Reg_energetich_b)\n",
    "model.regularizers.add(Reg_himich)\n",
    "model.regularizers.add(Reg_himich_b)\n",
    "model.regularizers.add(Reg_legkaya)\n",
    "model.regularizers.add(Reg_legkaya_b)\n",
    "model.regularizers.add(Reg_lesopromishl)\n",
    "model.regularizers.add(Reg_lesopromishl_b)\n",
    "model.regularizers.add(Reg_med)\n",
    "model.regularizers.add(Reg_med_b)\n",
    "\n",
    "model.regularizers.add(Reg_neftegaz)\n",
    "model.regularizers.add(Reg_neftegaz_b)\n",
    "model.regularizers.add(Reg_pharm)\n",
    "model.regularizers.add(Reg_pharm_b)\n",
    "model.regularizers.add(Reg_pish_i_pererab)\n",
    "model.regularizers.add(Reg_pish_i_pererab_b)\n",
    "model.regularizers.add(Reg_vooruzh)\n",
    "model.regularizers.add(Reg_vooruzh_b)\n",
    "\n",
    "model.regularizers.add(Reg_radioelektron)\n",
    "model.regularizers.add(Reg_radioelektron_b)\n",
    "model.regularizers.add(Reg_selskoxoz_i_lesnoe)\n",
    "model.regularizers.add(Reg_selskoxoz_i_lesnoe_b)\n",
    "model.regularizers.add(Reg_stankoinstrument)\n",
    "model.regularizers.add(Reg_stankoinstrument_b)\n",
    "model.regularizers.add(Reg_stroit)\n",
    "model.regularizers.add(Reg_stroit_b)\n",
    "model.regularizers.add(Reg_sudostroit)\n",
    "model.regularizers.add(Reg_sudostroit_b)\n",
    "model.regularizers.add(Reg_transportn)\n",
    "model.regularizers.add(Reg_transportn_b)\n",
    "model.regularizers.add(Reg_tsvetn_metallurg)\n",
    "model.regularizers.add(Reg_tsvetn_metallurg_b)\n",
    "model.regularizers.add(Reg_tyazh_mashinostr)\n",
    "model.regularizers.add(Reg_tyazh_mashinostr_b)\n",
    "print 'Regularizers done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   !!!!!! ====   RUN TEST  ====  !!!!!! Valid!!! ====================\n",
    "\n",
    "tau=1e+7\n",
    "data_dir='.\\\\COPY_import_branches_dictionaries\\\\'\n",
    "files = glob.glob(\"COPY_import_branches_dictionaries/*.txt\")\n",
    "file_list=[22]\n",
    "for imp_file in files:\n",
    "    file_name=imp_file.split(\"\\\\\")[1]\n",
    "    file_list.append(file_name)\n",
    "        \n",
    "background_topics = []\n",
    "domain_topics = []\n",
    "all_topics = []\n",
    "\n",
    "for i in range(1, 57):\n",
    "    if i <= 22:\n",
    "        topic_name = \"d\" + str(i)\n",
    "        domain_topics.append(topic_name)\n",
    "    else:\n",
    "        topic_name = \"b\" + str(i)\n",
    "        background_topics.append(topic_name)\n",
    "    all_topics.append(topic_name)\n",
    "\n",
    "model=artm.ARTM(topic_names = all_topics,\n",
    "            dictionary=main_dictionary, \n",
    "            cache_theta=True,\n",
    "            class_ids={'text': 1.0, 'bigrams':5.0}, #, 'import': 10.0}, #'mpk_main':1.5, 'mpk_full':2.0},\n",
    "            theta_columns_naming='title',\n",
    "            seed=-1)\n",
    "model.initialize(dictionary=main_dictionary) \n",
    "    \n",
    "Set_Multiple_Scores()\n",
    "set_model_scores(model)\n",
    "print 'Model is built, waiting for regularizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "print 'tau='+str(tau)\n",
    "for i in range(1,20):\n",
    "    print 'step ' + str(i-1)+'-'+str(i)+':'\n",
    "    FitModel()\n",
    "\n",
    "print '=========='\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitModel():\n",
    "    \n",
    "    sparsed_count_t=0\n",
    "    sparsed_count_b=0\n",
    "    model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=2)\n",
    "\n",
    "    for i in range(1,23):\n",
    "        \n",
    "        SparsityScore_t=model.score_tracker['SparsityPhiScore_text'+str(i)].last_value\n",
    "        SparsityScore_b=model.score_tracker['SparsityPhiScore_bigrams'+str(i)].last_value\n",
    "        print SparsityScore_t\n",
    "        if SparsityScore_t>0.990:\n",
    "            model.regularizers['Smoth_t'+str(i)].tau = 1\n",
    "            sparsed_count_t+=1\n",
    "            #print 'rewrite reg'+str(i)+' to ' + str(model.regularizers['Smoth_t'+str(i)].tau)\n",
    "        if SparsityScore_b>0.990:\n",
    "            model.regularizers['Smoth_b'+str(i)].tau = 1\n",
    "            sparsed_count_b+=1\n",
    "    print 'Sparsed count text: ' + str(sparsed_count_t)\n",
    "    print 'Sparsed count bigrams: ' + str(sparsed_count_b)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_figures(model, tau_value):\n",
    "    # perplexity\n",
    "    x = range(model.num_phi_updates)[1:]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    plt.title(u'Метрики качества модели, '+ '$\\\\tau$='+ str(format(tau_value, \".0e\")), fontsize=14, y=1.06)\n",
    "    \n",
    "    ax1.plot(x, model.score_tracker['PerplexityScore'].value[1:], 'g-', linewidth=1, label=u\"Перплексия\")\n",
    "    ax1.set_xlabel(u'Номер итерации')\n",
    "    ax1.set_ylabel(u'Перплексия', color='g')\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    ax2.plot(x, model.score_tracker['SparsityPhiScore_text'].value[1:], 'r*', linewidth=1, label=u'Разреженность '+'$\\\\Phi$'+'-text')\n",
    "    ax2.plot(x, model.score_tracker['SparsityPhiScore_bigrams'].value[1:], 'r:', linewidth=1, label=u'Разреженность '+'$\\\\Phi$'+'-bigrams')\n",
    "    ax2.plot(x, model.score_tracker['SparsityThetaScore'].value[1:], 'r-.', linewidth=1, label=u'Разреженность '+'$\\\\Theta$')\n",
    "    ax2.set_ylabel(u'Доля', color='r')\n",
    "    ax2.legend(bbox_to_anchor=(1.10, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.3)\n",
    "    \n",
    "    \n",
    "    ax1.text(0.14, -0.16, u'Перплексия: ' + str(round(model.score_tracker['PerplexityScore'].last_value, 3))+\n",
    "             u'\\nРазреженность ' +'$\\\\Phi$'+'-text: ' + str(round(model.score_tracker['SparsityPhiScore_text'].last_value, 3))+\n",
    "             u',  Разреженность ' +'$\\\\Phi$'+'-bigrams: ' +str(round(model.score_tracker['SparsityPhiScore_bigrams'].last_value, 3))+\n",
    "             u'\\nРазреженность ' +'$\\\\Theta$: '+str(round(model.score_tracker['SparsityThetaScore'].last_value,3)), \n",
    "         transform=plt.gcf().transFigure, bbox=props)\n",
    "\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.savefig('scores_sparsity.png', dpi=150, bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "    \n",
    "    # kernels\n",
    "    x = range(model.num_phi_updates)[1:]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(x, model.score_tracker['DomainTopicKernelScore'].average_size[1:], 'g-', linewidth=1, label=u\"Размер ядра\")\n",
    "    ax1.set_xlabel(u'Номер итерации')\n",
    "    ax1.set_ylabel(u'Размер ядра', color='g')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, model.score_tracker['DomainTopicKernelScore'].average_contrast[1:], 'r*', linewidth=1, label=u\"Контраст\")\n",
    "    ax2.plot(x, model.score_tracker['DomainTopicKernelScore'].average_purity[1:], 'r--', linewidth=1, label=u\"Чистота\")\n",
    "    ax2.set_ylabel(u'Доля', color='r')\n",
    "    ax2.legend(bbox_to_anchor=(1.10, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    ax1.text(0.14, -0.07, u'Размер ядра: ' + str(round(model.score_tracker['DomainTopicKernelScore'].last_average_size, 3))+\n",
    "             u',  Контраст: ' + str(round(model.score_tracker['DomainTopicKernelScore'].last_average_contrast, 3))+\n",
    "             u',  Чистота: ' + str(round(model.score_tracker['DomainTopicKernelScore'].last_average_purity, 3)),\n",
    "         transform=plt.gcf().transFigure, bbox=props)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    \n",
    "    \n",
    "    plt.savefig('scores_kernel.png', dpi=150, bbox_inches = 'tight')\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_figures(model, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = model.get_phi(class_ids = ['text', 'bigrams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Печатает, в каких темах встретились импорт-слова\n",
    "def show_topics_with_highlihts(phi, model, topics, top, import_dictionary):\n",
    "    \n",
    "    for topic in topics:\n",
    "        \n",
    "        phi = phi.sort_values(topic, ascending=False)\n",
    "        rows_names_list=phi[topic][:top].index.tolist()\n",
    "        \n",
    "        words_list = [s[1] for s in rows_names_list]\n",
    "        \n",
    "        interesting_words=[]\n",
    "        print topic\n",
    "        #print '|'.join(words_list)\n",
    "        print \"---  Import words:  ---\"\n",
    "        for branch, import_words in import_dictionary.items():\n",
    "            for import_word in import_words.split(' '):\n",
    "                if import_word.decode('utf8') in words_list:\n",
    "                    print branch, import_word\n",
    "                    interesting_words.append(import_word)\n",
    "        #if len(interesting_words)>0:\n",
    "        print '|'.join(words_list)\n",
    "       # print \"---  Import words:  ---\"\n",
    "       # print ' '.join(interesting_words)\n",
    "            \n",
    "        print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phi = model.get_phi(class_ids = ['text', 'bigrams'])  #, topic_names=domain_topics)\n",
    "\n",
    "#  Импортируем список импорт слов\n",
    "# data_dir='.\\\\import_batches\\\\'\n",
    "# data_file='import_dictionary_for_pandas.txt'\n",
    "# header=['token', 'class_id', 'value', 'tf', 'df']\n",
    "# import_pd = pd.read_csv(data_dir+data_file, delimiter=',', names=header, encoding=\"utf8\")\n",
    "# import_words=import_pd['token'].tolist()\n",
    "\n",
    "import codecs\n",
    "data_dir='.\\\\keyword_search\\\\'\n",
    "all_import_file='import_words.txt'\n",
    "\n",
    "\n",
    "with open(data_dir+all_import_file, 'r' ) as f:\n",
    "    import_dicionary={}\n",
    "    import_lines = f.readlines()\n",
    "    for line in import_lines:\n",
    "        line_list=line.split('|')\n",
    "        import_dicionary[line_list[0]]=line_list[1]\n",
    "    \n",
    "show_topics_with_highlihts(phi, model, all_topics, 10, import_dicionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = model.get_theta()\n",
    "theta_tr = theta.transpose(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_dir='.\\\\input_data\\\\'\n",
    "data_file='all_docs_similar_columns_count.txt'\n",
    "\n",
    "header=['id','status','appl_num','mpk','date_appl','date_publ','author','assignee','title','abstract']\n",
    "patent_raw = pd.read_csv(data_dir+data_file, delimiter='-', index_col=0, names=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta_tr2 = theta_tr.loc[~theta_tr.duplicated()]\n",
    "# theta_tr.shape[0]\n",
    "\n",
    "\n",
    "theta_tr.index = theta_tr.index.astype(int) # str -> int\n",
    "#patent_raw.index = patent_raw.index.astype(int)\n",
    "#abstract_theta = pd.concat([patent_raw, theta_tr], axis=1) # merge\n",
    "abstract_theta=theta_tr.join(patent_raw)\n",
    "abstract_theta.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_target_patents(topics_list, articles_n, theta_df):\n",
    "    data_dir='.\\\\output\\\\1e+7\\\\'\n",
    "    header=['topic', 'status','appl_num','date_appl','date_publ','author','assignee','title','abstract']\n",
    "    for topic in topics_list:\n",
    "        data_file=topic+'_patents_1e+7.txt'\n",
    "        res_theta_df = theta_df[theta_df[topic] > 0.6]\n",
    "        #res_theta_df = theta_df.nlargest(articles_n, topic, keep='all')\n",
    "        res_theta_df=res_theta_df[[topic,'status','appl_num','mpk','date_appl','date_publ','author','assignee','title','abstract']]\n",
    "        \n",
    "        res_theta_df.to_csv(data_dir+data_file, sep='-', index=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_terms(topics_list, keywords_n, phi_df):\n",
    "    data_dir='.\\\\output\\\\1e+7\\\\'\n",
    "    data_file='keywords.txt'\n",
    "    \n",
    "    import_dir='.\\\\COPY_import_branches_dictionaries\\\\'\n",
    "    files = glob.glob(\"COPY_import_branches_dictionaries/*.txt\")\n",
    "    file_list=[]\n",
    "    for imp_file in files:\n",
    "        file_name=imp_file.split(\"\\\\\")[1]\n",
    "        file_list.append(file_name)\n",
    "    \n",
    "#     keyword_pd = pd.DataFrame(columns=['file', 'topic', 'keywords'])\n",
    "#     keyword_pd['topic']=topics_list\n",
    "#     keyword_pd['file']=file_list[1:]\n",
    "#     for topic in topics_list:\n",
    "#         res_phi = phi_df.nlargest(keywords_n, topic, keep='all')\n",
    "#         rows_names_list=res_phi.index.tolist()\n",
    "#         res_phi = [s[1] for s in rows_names_list]\n",
    "#         topics_list.index(topic)\n",
    "#         keyword_pd.iloc[topics_list.index(topic), 2]=\", \".join(res_phi)\n",
    "#     keyword_pd.to_csv(data_dir+data_file, sep='-', index=False, header=True, encoding='utf-8')\n",
    "    \n",
    "    keyword_pd = pd.DataFrame([file_list], columns=topics_list)\n",
    "    \n",
    "    temp_pd=pd.DataFrame(columns=topics_list)\n",
    "    \n",
    "    for topic in topics_list:\n",
    "        res_phi = phi_df.nlargest(keywords_n, topic, keep='all')\n",
    "        rows_names_list=res_phi.index.tolist()\n",
    "        res_phi = [s[1] for s in rows_names_list]\n",
    "        temp_pd[topic]=res_phi\n",
    "    keyword_pd = pd.concat([keyword_pd,temp_pd])\n",
    "    keyword_pd.to_csv(data_dir+data_file, sep='-', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_target_patents(domain_topics, 40, abstract_theta)\n",
    "#save_terms(domain_topics, 15, phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tau1e+7_bigrams3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
